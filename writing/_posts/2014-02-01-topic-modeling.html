---
layout: post
title: "Topic Modeling"
tags: computing digital-humanities
---

<p>
	In 2014, I did some work as a research assistant to <a href="https://www.mpiwg-berlin.mpg.de/people/ebenson">Etienne Benson</a> exploring how we might use software to interpret changing discourses about the environment. Etienne ultimately didn’t use these digital humanities techniques in his book on the topic, <cite><a href="https://press.uchicago.edu/ucp/books/book/chicago/S/bo50271092.html">Surroundings: A History of Environments and Environmentalisms</a></cite>, but in the process I wrote this report on the potential of topic modeling specifically.
</p>

<p>
	• • •
</p>

<p>
	Although it has a longer history, topic modeling is now typically done using Latent Dirichlet Allocation, a hierarchical Bayesian model developed by computer scientist David Blei and his colleagues (<a href="http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf">initial article</a>, <a href="http://www.cs.princeton.edu/~blei/papers/BleiLafferty2009.pdf">technical description</a>, <a href="http://www.cs.princeton.edu/~blei/papers/Blei2012.pdf">less technical description</a>). The premise, it’s worth stating explicitly, is a rather odd model of composition. “Documents are mixtures of topics,” explains <a href="http://psiexp.ss.uci.edu/research/papers/SteyversGriffithsLSABookFormatted.pdf">one article</a>, “where a topic is a probability distribution over words. A topic model is a generative model for documents: it specifies a simple probabilistic procedure by which documents can be generated. To make a new document, one chooses a distribution over topics. Then, for each word in that document, one chooses a topic at random according to this distribution, and draws a word from that topic. Standard statistical techniques can be used to invert this process, inferring the set of topics that were responsible for generating a collection of documents.” So topic modeling involves reverse engineering the creation of a text, which is conceptualized as a process of synthesizing multiple topics into a single document. As far as the computer can understand them, though, these “topics” are simply sets of words with associated frequencies. Because topics can be conceptualized as vectors, it is also relatively easy to measure correlations between words or correlations between topics.
</p>
<p>
	Among digital humanists, the most widely used tool for topic modeling is <a href="http://mallet.cs.umass.edu/">MALLET</a> (<a href="http://programminghistorian.org/lessons/topic-modeling-and-mallet">tutorial</a>), “a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.” There are several alternatives, though, including libraries for <a href="http://www.cs.princeton.edu/~blei/lda-c/">C</a>, <a href="http://arbylon.net/projects/">Java</a>, <a href="http://radimrehurek.com/gensim/">Python</a>, <a href="http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm">Matlab</a>, and R (<a href="http://www.jstatsoft.org/v40/i13/">topicmodels</a>, <a href="http://cran.r-project.org/web/packages/lda">lda</a>) as well as two Java applications: <a href="https://code.google.com/p/topic-modeling-tool/">Topic Modeling Tool</a> provides a graphical interface for MALLET (<a href="http://miriamposner.com/blog/very-basic-strategies-for-interpreting-results-from-the-topic-modeling-tool/">tutorial</a>), while the <a href="http://nlp.stanford.edu/downloads/tmt/tmt-0.4/">Stanford Modeling Toolbox</a> is scriptable using Scala  and processes CSV files. The Maryland Institute for Technology in the Humanities has produced some <a href="http://mith.umd.edu/topic-modeling-round-up-and-some-new-software/">additional utilities</a> for MALLET. Perhaps the easiest tool available is the Zotero extension <a href="http://papermachines.org/">Paper Machines</a>, which incorporates MALLET. Paper Machines can model not only Zotero collections, but also JSTOR Data for Research CSV files, which otherwise requires <a href="http://www.jgoodwin.net/?p=1036">some preprocessing</a> to be legible to MALLET. Its output doesn’t seem very versatile, though.
</p>
<p>
	There are several precedents for tracking the rise and fall of topics over time, though few seem to have contributed to peer-reviewed historical articles. A project by historian Robert Nelson on the <a href="http://dsl.richmond.edu/dispatch/">Richmond <cite>Daily Dispatch</cite></a> from 1860 to 1865, conducted using MALLET, produced a widely-cited web presentation, and computer scientist David Newman and historian Sharon Block conducted a similar study of the <a href="http://www.ics.uci.edu/~newman/pubs/JASIST_Newman.pdf"><cite>Pennsylvania Gazette</cite></a> from 1728 to 1800. (Block and Newman also published a historical article on <a href="muse.jhu.edu/journals/journal_of_womens_history/summary/v023/23.1.block.html">trends in the field of women’s history</a> from 1985 to 2005 for which they used topic modeling to compare the prominence of research topics, but its diachronic analysis is all based on text-mining.) Historian Cameron Belvins has diachronically topic modeled <a href="http://historying.org/2010/04/01/topic-modeling-martha-ballards-diary/">Martha Ballard’s diary</a>. Among several studies of academic journals by computer scientists, David Mimno’s work on the <a href="http://www.perseus.tufts.edu/publications/02-jocch-mimno.pdf">field of classics</a> stands out for its careful methodology.
</p>
<p>
	Perhaps the best model for using topic modeling in historical research is literature scholar Allen Riddell’s apparently unpublished paper on the <a href="http://ariddell.org/static/how-to-read-n-articles.pdf">history of German Studies</a>, which contains a particularly clear technical explanation as well as a study of the rise and fall of topics ranging from gender to Goethe. Similarly, but in published articles, folklorist John Laudun and literature scholar Jonathan Goodwin used topic modeling to illuminate the dynamics of <a href="http://muse.jhu.edu/journals/journal_of_american_folklore/v126/126.502.laudun.html">folklore studies’ turn toward performance</a>, while Andrew Goldstone and Ted Underwood not only modeled the <a href="http://journalofdigitalhumanities.org/2-1/what-can-topic-models-of-pmla-teach-us-by-ted-underwood-and-andrew-goldstone/">topics of the <cite>Proceedings of the Modern Language Association</cite></a>, but also produced network visualizations of the relationships between the topics.
</p>
<p>
	It is also possible to produce models in which  topics themselves evolve over time, changing their word composition. As a demonstration of <a href="http://www.cs.princeton.edu/~blei/papers/BleiLafferty2006a.pdf">dynamic topic models</a>, Blei and fellow computer scientist John Lafferty produced a <a href="http://topics.cs.princeton.edu/Science/">browsable year-by-year model of <cite>Science</cite></a>. Blei has also published, with computer scientist Sean M. Gerrish, a study attempting to use such models to <a href="http://www.cs.princeton.edu/~blei/papers/GerrishBlei2010.pdf">measure the influence of scientific publications</a> without resort to citations or other conventional bibliometric tools. Similarly, computer scientists Xuerui Wang and Andrew McCallum (the latter the initial developer of MALLET) developed a model called <a href="http://people.cs.umass.edu/~mccallum/papers/tot-kdd06.pdf">Topics over Time</a>, which they tested on Presidential State of the Union Addresses and proceedings of the Neural Information Processing Systems conference. Very few humanists—possibly only Laudun and Goodwin—have used these tools.
</p>